<!doctype html>
<html>
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Vulnerability Scanners</title>
  <meta name="generator" content="CherryTree">
  <link rel="stylesheet" href="res/styles3.css" type="text/css" />
</head>
<body>
<div class='page'><strong><h1># Vulnerability Scanners</h1></strong><br />These are automatic tools that will scan and site for you and attempt to identify vulnerabilities.<br />Sometimes they yield results, sometimes they don&#39;t. It&#39;s worth trying, but just don&#39;t count on it.<br /><br /><strong><h2>## Tips</h2></strong><br />When testing large environments, you will often find that URL paths provide access to different server components. <br />e.g. The root directory <code>/</code> request returns ASP.NET 2.0.50727, <br />but the <code>/en-gb/default.aspx</code> request returns ASP.NET 4.0.30319.<br /><br /><strong><h2>## Nikto</h2></strong><br /><code>nikto -o nikto.txt -h </code><a href="http://www.url.com/">http://www.url.com/</a><br /><br /><code>-h</code>      host<br /><code>-p</code>      port (default is 80)<br /><code>-o</code>      output<br /><br /><strong><h2>## uniscan</h2></strong><br />Uniscan is a simple Remote File Include, Local File Include and Remote Command Execution vulnerability scanner (mainly and LFI scanner).<br /><a href="https://tools.kali.org/web-applications/uniscan">https://tools.kali.org/web-applications/uniscan</a><br /><br />OPTIONS:<br />-h  help<br />-u  &lt;url&gt;<br />-q  Enable Directory checks<br />-w  Enable File checks<br />-e  Enable robots.txt and sitemap.xml check<br />-d  Enable Dynamic checks<br />-s  Enable Static checks<br /><br /><code>uniscan -u http://www.example.com/ -qweds</code><br /><br />Reports get saved into:<br /><code>/usr/share/uniscan/report</code></div>
</body>
</html>
